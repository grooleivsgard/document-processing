{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import codecs\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.probability import FreqDist\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "\n",
    "# 1.0 - random number generator\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition document: This function partitions the document by paragraphs into seperate documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Partition document collection\n",
    "def partition(input_file):\n",
    "    # 1.1 - open and load utf-8 encoded file\n",
    "    with codecs.open(input_file, \"r\", \"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        chunks = []\n",
    "        paragraph = []\n",
    "        # 1.2 - Partition file into seperate paragraphs. Each paragraph will be a seperate document.\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:  # if the line is not empty, add to current paragraph\n",
    "                paragraph.append(line)\n",
    "            else:  # if the line is empty and there's an existing paragraph, end the current paragraph\n",
    "                if paragraph:\n",
    "                    chunks.append(' '.join(paragraph))\n",
    "                    paragraph = []\n",
    "        # Add the last paragraph if the file doesn't end with an empty line\n",
    "        if paragraph:\n",
    "            chunks.append(' '.join(paragraph))\n",
    "\n",
    "    for index, paragraph in enumerate(chunks, start=1):\n",
    "        output_filename = f\"paragraph_{index}.txt\"\n",
    "        with codecs.open(output_filename, 'w', \"utf-8\") as out_file:\n",
    "            out_file.write(paragraph)\n",
    "\n",
    "    print(f\"Partitioned {len(chunks)} into seperate files.\")\n",
    "\n",
    "\n",
    "def preprocess_collection(target_word, directory_path):\n",
    "    files_in_directory = os.listdir()\n",
    "    collection = [f for f in files_in_directory if\n",
    "                  os.path.isfile(os.path.join(directory_path, f)) and f.startswith(\"paragraph_\") and f.endswith(\n",
    "                      \".txt\")]\n",
    "\n",
    "    # 1.3 - Remove paragraph (document) if it contains target word\n",
    "    for file in collection:\n",
    "        with open(os.path.join(directory_path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            if target_word in content:\n",
    "                os.remove(os.path.join(directory_path, file))\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess collection: This function removes all paragraphs containing the word \"Gutenberg\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_collection(target_word, directory_path):\n",
    "    files_in_directory = os.listdir()\n",
    "    collection = [f for f in files_in_directory if\n",
    "                  os.path.isfile(os.path.join(directory_path, f)) and f.startswith(\"paragraph_\") and f.endswith(\n",
    "                      \".txt\")]\n",
    "\n",
    "    # 1.3 - Remove paragraph (document) if it contains target word\n",
    "    for file in collection:\n",
    "        with open(os.path.join(directory_path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            if target_word in content:\n",
    "                os.remove(os.path.join(directory_path, file))\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize: One function accepts a collection as parameter, and iterates through the collection by calling the function tokenize_doc, which accepts a document as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Tokenize document collection\n",
    "def tokenize(collection):\n",
    "    processed_files = []\n",
    "\n",
    "    # Initialize FreqDist\n",
    "    fdist = FreqDist()\n",
    "\n",
    "    # Read files\n",
    "    for file in collection:\n",
    "        tokenized_file = tokenize_doc(file)\n",
    "\n",
    "        # 1.7 - Add processed tokens to list and update fdist\n",
    "        fdist.update(tokenized_file)\n",
    "        processed_files.append(tokenized_file)\n",
    "\n",
    "    return processed_files\n",
    "\n",
    "def tokenize_doc(file):\n",
    "    # 4.1 - Apply transformations to query\n",
    "    punctuation = [',', '.', ';', ':', '?', '!', '(', ')', '[', ']', '{', '}', '\"', \"'\", \"â€™\"]\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        # 1.5 - Convert to lower case\n",
    "        content = f.read().lower()\n",
    "\n",
    "        # 1.4 - Tokenize words\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "\n",
    "        # 1.6 - Stem tokens and remove punctuation (1.5)\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        stemmed_file = [stemmer.stem(word) for word in tokens if\n",
    "                        word not in punctuation]\n",
    "        tokenized_doc = stemmed_file\n",
    "\n",
    "        return tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dictionary building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three functions are used to build a dictionary based on the processed files and stopwords, which in turn is used to create a bag-of-words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(processed_files):\n",
    "    # 2.1 - Build the dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(processed_files)\n",
    "\n",
    "    # Retrieve and filter stopwords\n",
    "    stopwords = retrieve_stopwords()\n",
    "    stop_ids = []\n",
    "    removed = 0\n",
    "    for word in stopwords:\n",
    "        try:\n",
    "            stop_id = dictionary.token2id[word]\n",
    "            stop_ids.append(stop_id)\n",
    "            removed += 1\n",
    "        except:\n",
    "            pass\n",
    "    dictionary.filter_tokens(stop_ids)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def to_bow(processed_files, dictionary):\n",
    "    # 2.2 - Map paragraphs into Bags-of-Words\n",
    "    bow_corpus = [dictionary.doc2bow(token, allow_update=True) for token in processed_files]\n",
    "    return bow_corpus\n",
    "\n",
    "\n",
    "def retrieve_stopwords():\n",
    "    # File downloaded from TextFixer: https://www.textfixer.com/tutorials/common-english-words.php\n",
    "    file = open('./common-english-words.txt', 'r')\n",
    "    stopwords = []\n",
    "    for rows in file:\n",
    "        row = rows.rstrip().split(',')\n",
    "        stopwords += row\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity function below creates the TF-IDF model from the bag-of-words corpus, which is used to initialize the LSI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(query, collection):\n",
    "    # 2.1 - Build dictionary\n",
    "    dictionary = build_dictionary(collection)\n",
    "\n",
    "    # 2.2 - Map paragraphs to Bag-of-Words\n",
    "    corpus = to_bow(collection, dictionary)\n",
    "\n",
    "    # -- TD-IDF conversion --\n",
    "    # 3.1 - Initialize TD-IDF model using Bag-of-Words\n",
    "    tfidf_model = gensim.models.TfidfModel(corpus, normalize=True)\n",
    "    # 3.2 - Map bow into TF-IDF weights\n",
    "    tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "    # --- LSI model ---\n",
    "    # 3.4 - Initialize LSI model using the TD-IDF corpus\n",
    "    lsi_model = gensim.models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=100)  # Create LSI Model\n",
    "    lsi_corpus = lsi_model[tfidf_corpus]\n",
    "\n",
    "    # ---- Model similarity ---\n",
    "    # 3.3 - Construct MatrixSimilarity object of the LSI corpus\n",
    "    index = gensim.similarities.MatrixSimilarity(lsi_corpus)\n",
    "\n",
    "    # 3.5 - Printing first 3 LSI topics\n",
    "    result = lsi_model.show_topics(3)\n",
    "    print(f\"LSI results: {result}\")\n",
    "    \n",
    "    # 4.2 - Convert the query to LSI space\n",
    "    query_vec = dictionary.doc2bow(query)\n",
    "    lsi_vec = lsi_model[query_vec]\n",
    "\n",
    "    # Perform similarity query against the corpus\n",
    "    sims = index[lsi_vec]\n",
    "    sims = sorted(enumerate(sims), key=lambda kv: -kv[1])[:3]\n",
    "\n",
    "    # 4.3 - Report top 3 most relevant paragraphs for the query\n",
    "    count = 0\n",
    "    for doc_position, doc_score in sims:\n",
    "        paragraph = retrieve_paragraph(doc_position)\n",
    "        print(f\"[paragraph {doc_position}] \\n {paragraph} \\n \")\n",
    "        count += 1\n",
    "        if count == 3:\n",
    "            break\n",
    "    \n",
    "def retrieve_paragraph(doc_position):\n",
    "    \"\"\"\n",
    "    Retrieves the original, unprocessed paragraph.\n",
    "    \"\"\"\n",
    "    directory_path = \"./\"\n",
    "    filename = f\"paragraph_{doc_position}.txt\"\n",
    "    filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = \"\"\n",
    "            for _ in range(5):\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                content += line\n",
    "            return content\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 - Results from printing the first 3 LSI topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI results: \n",
    "[\n",
    "    (0, '0.116*\"labour\" + 0.109*\"price\" + 0.105*\"their\" + 0.103*\"is\" + 0.102*\"employ\" + 0.101*\"hi\" + 0.101*\"produc\" + 0.101*\"countri\" + 0.100*\"it\" + 0.100*\"a\"'), \n",
    "    (1, '-0.287*\"labour\" + -0.203*\"rent\" + -0.187*\"stock\" + -0.186*\"land\" + -0.180*\"employ\" + -0.178*\"profit\" + -0.160*\"wage\" + -0.155*\"capit\" + -0.153*\"produc\" + 0.145*\"coloni\"'), \n",
    "    (2, '0.342*\"price\" + 0.276*\"silver\" + 0.203*\"quantiti\" + 0.201*\"gold\" + 0.173*\"coin\" + 0.161*\"valu\" + 0.149*\"corn\" + 0.144*\"money\" + -0.143*\"trade\" + -0.140*\"capit\"')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we can deduct the following information:\n",
    "\n",
    "    Topic 0: Focuses mostly on labor, price, and country-related aspects. Words like \"labour,\" \"price,\" and \"countri\" have relatively high weights, suggesting that these are important concepts within this topic.\n",
    "    \n",
    "    Topic 1: Seems to concentrate on aspects of labor, but also includes terms related to rent, stock, land, and profits. The term \"coloni\" has a negative weight, suggesting that it is not aligned with the other terms in this topic or possibly that it is more related to another topic.\n",
    "\n",
    "    Topic 2: Appears to be about currency and commodities, with \"price,\" \"silver,\" \"gold,\" and \"coin\" being heavily weighted terms. The negative weights for \"trade\" and \"capit\" (capital) suggest these terms are less relevant to this particular topic.\n",
    "\n",
    "This indicates that the numbers before each word (e.g., 0.116*\"labour\") are weights that signify the importance of each word in defining the respective topic. A higher absolute value of the weight indicates higher relevance to the topic. Negative values often indicate that the word is relevant but in a different context or opposite sense in comparison to other words in the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- PART 4: Querying ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_query(query):\n",
    "    \"\"\"\n",
    "    Converts a query to a file, and tokenizes file.\n",
    "    \"\"\"\n",
    "    # 4.1 - Preprocess query to remove stopwords, punctuation, tokenize and stem\n",
    "    filename = query_to_file(query)\n",
    "    directory_path = \"./\"\n",
    "    if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "        processed_query = filename\n",
    "    else:\n",
    "        processed_query = ''\n",
    "\n",
    "    return processed_query\n",
    "\n",
    "\n",
    "def query_to_file(query):\n",
    "    \"\"\"\n",
    "    Writes query to a file and returns the filename query.txt\n",
    "    \"\"\"\n",
    "    filename = \"query.txt\"\n",
    "    mode = \"w\" if os.path.exists(filename) else \"x\"\n",
    "\n",
    "    with open(filename, mode) as query_file:\n",
    "        query_file.write(query)\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Results from top 3 the most relevant paragraphs for the query \"What is the function of money?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[paragraph 248] \n",
    " That wealth consists in money, or in gold and silver, is a popular notion which naturally arises from the double function of money, as the instrument of commerce, and as the measure of value. In consequence of its being the instrument of commerce, when we have money we can more readily obtain whatever else we have occasion for, than by means of any other commodity. The great affair, we always find, is to get money. When that is obtained, there is no difficulty in making any subsequent purchase. In consequence of its being the measure of value, we estimate that of all other commodities by the quantity of money which they will exchange for. We say of a rich man, that he is worth a great deal, and of a poor man, that he is worth very little money. A frugal man, or a man eager to be rich, is said to love money; and a careless, a generous, or a profuse man, is said to be indifferent about it. To grow rich is to get money; and wealth and money, in short, are, in common language, considered as in every respect synonymous. \n",
    " \n",
    "[paragraph 807] \n",
    " It would be too ridiculous to go about seriously to prove, that wealth does not consist in money, or in gold and silver; but in what money purchases, and is valuable only for purchasing. Money, no doubt, makes always a part of the national capital; but it has already been shown that it generally makes but a small part, and always the most unprofitable part of it. \n",
    " \n",
    "[paragraph 1487] \n",
    " When, by any particular sum of money, we mean not only to express the amount of the metal pieces of which it is composed, but to include in its signification some obscure reference to the goods which can be had in exchange for them, the wealth or revenue which it in this case denotes, is equal only to one of the two values which are thus intimated somewhat ambiguously by the same word, and to the latter more properly than to the former, to the moneyâ€™s worth more properly than to the money. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filename = \"pg3300.txt\"\n",
    "    target_word = \"Gutenberg\"\n",
    "    directory_path = \"./\"\n",
    "    query1 = \"What is the function of money?\"\n",
    "    query2 = \"How taxes influence Economics?\"\n",
    "    # processor = DataProcessing(input_filename)\n",
    "\n",
    "    # ONLY FOR TESTING - Remove all generated files\n",
    "    # delete_all_files()\n",
    "\n",
    "    # STEP 1 - Partition all chunks into list of paragraphs\n",
    "    paragraphs = partition(filename)\n",
    "\n",
    "    # STEP 2 - Filter collection\n",
    "    #processed_collection = preprocess_collection(target_word, directory_path)\n",
    "    #processed_query = preprocess_query(query1)\n",
    "\n",
    "    # STEP 3 - Tokenize\n",
    "    #tokenized_collection = tokenize(processed_collection)\n",
    "    #tokenized_query = tokenize_doc(processed_query)\n",
    "\n",
    "    # STEP 4 - Run similarity check\n",
    "    #similarity(tokenized_query, tokenized_collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groelisabeth-j2fBgUnB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
